{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4217b-da77-4dd3-88f9-34514cf83860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies\n",
    "!pip install -q jedi\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q llama-index-embeddings-huggingface\n",
    "!pip install -q llama-index llama-index-llms-google-genai\n",
    "!pip install -q llama-index-readers-file\n",
    "!pip install -q pymupdf\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q llama-index-vector-stores-faiss\n",
    "!pip install -q gradio-pdf\n",
    "!pip install -q numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb76e5-4b40-4ae2-a402-b910268ff966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown,display\n",
    "import fitz\n",
    "import faiss\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import gradio as gr\n",
    "from llama_index.core import Document,VectorStoreIndex,SimpleDirectoryReader,ServiceContext,Settings,StorageContext\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "#from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766e101-756a-4b7e-b69b-223bf76392cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb966da-3737-4922-85e8-f1887e6433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gemini = GoogleGenAI(model=\"models/gemini-2.5-flash\")\n",
    "genai.configure(\"YOUR_API_KEY_HERE\")\n",
    "Settings.llm = llm_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf8ac3-b792-4437-8e06-f3647fd92b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageInfo:\n",
    "  \"\"\"Stores information about a single page\"\"\"\n",
    "  page_num: int\n",
    "  text: str\n",
    "  doc_type: Optional[str] = None\n",
    "@dataclass\n",
    "class LogicalDocument:\n",
    "  \"\"\"Represents a logical document within a PDF\"\"\"\n",
    "  doc_id: str\n",
    "  doc_type: str\n",
    "  page_start: int\n",
    "  page_end: int\n",
    "  text: str\n",
    "  chunks: List[Dict] = None\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "  \"\"\"Metadata for each chunk\"\"\"\n",
    "  chunk_id: str\n",
    "  doc_id: str\n",
    "  doc_type: str\n",
    "  chunk_index: str\n",
    "  page_start: int\n",
    "  page_end: int\n",
    "  text: str\n",
    "  embedding: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed0f77-c5f0-4944-a7cd-9c9258596246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the document type based on its content.\n",
    "def classify_document(text:str, max_length: int = 500) -> str:\n",
    "\n",
    "  text_sample = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "  You are a document type classifier.Analyze this document and classify it into ONE of these categories : contract,fees worksheet,resume,payslip,other.\n",
    "  Do not display the contents of the text.\n",
    "  Document:\n",
    "  {text_sample}\n",
    "  Respond with ONLY the category name, nothing else.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = llm_gemini.complete(prompt)\n",
    "    doc_type = response.text.lower().strip().rstrip(\".\")\n",
    "    valid_types = ['Resume','Contract','Fees Worksheet','PaySlip','Other']\n",
    "    for valid_type in valid_types:\n",
    "      if doc_type.lower() == valid_type.lower():\n",
    "        return valid_type\n",
    "    return 'Other'\n",
    "\n",
    "    return doc_type\n",
    "  except Exception as e:\n",
    "    print(f\"Error {str(e)}\")\n",
    "    return \"other\"\n",
    "# Function to detect document boundary\n",
    "  def is_same_document(prev_text: str, curr_text: str, doc_type: str = None) -> bool:\n",
    "    if not prev_text or not curr_text:\n",
    "        return False\n",
    "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
    "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
    "    prompt = f\"\"\"\n",
    "    Determine if these two pages are from the SAME document.\n",
    "\n",
    "    Current document type: {current_doc_type or 'Unknown'}\n",
    "    End of Previous Page:\n",
    "    ... {prev_sample}\n",
    "\n",
    "    Start of Current Page:\n",
    "    {curr_sample} ...\n",
    "\n",
    "    Consider:\n",
    "    - Continuity of content\n",
    "    - Formatting consistency\n",
    "    - Topic coherence\n",
    "    - Page numbers or headers\n",
    "\n",
    "    Answer ONLY 'Yes' if same document or 'No' if different document.\n",
    "  \"\"\"\n",
    "    try:\n",
    "      response = llm_gemini.complete(prompt)\n",
    "      return response.text.strip().lower().startswith(\"yes\")\n",
    "    except Exception as e:\n",
    "      print(f\"Boundary detection error: {e}\")\n",
    "      return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda3a1f-69a3-4e0b-ac73-e6b9bc25140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_analyze_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, classify pages by doc_type, group them into logical documents,\n",
    "    and return page-level info and logical document groupings.\n",
    "    \"\"\"\n",
    "    print(\"üìñ Starting PDF extraction and analysis...\")\n",
    "\n",
    "    # Load PDF\n",
    "    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n",
    "        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n",
    "    elif hasattr(pdf_file, \"read\"):\n",
    "        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
    "    else:\n",
    "        doc = fitz.open(pdf_file)\n",
    "\n",
    "    # Extract page text with optional OCR\n",
    "    pages_info = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if not text.strip():\n",
    "            print(f\"  Page {i}: No text found, skipping...\")\n",
    "            continue\n",
    "        # Classify the document type\n",
    "        doc_type = classify_document(text)\n",
    "        pages_info.append(PageInfo(page_num=i, text=text, doc_type=doc_type))\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    if not pages_info:\n",
    "        raise ValueError(\"No text could be extracted from PDF\")\n",
    "\n",
    "    print(f\"‚úÖ Extracted {len(pages_info)} pages\")\n",
    "\n",
    "    # Group consecutive pages by doc_type\n",
    "    current_doc_type = None\n",
    "    doc_counter = 0\n",
    "\n",
    "    for i, page in enumerate(pages_info):\n",
    "        if i == 0:\n",
    "            current_doc_type = page.doc_type\n",
    "        else:\n",
    "            if page.doc_type != current_doc_type:\n",
    "                doc_counter += 1\n",
    "                current_doc_type = page.doc_type\n",
    "\n",
    "        # Assign doc_id and page_in_doc\n",
    "        page.doc_id = f\"doc_{doc_counter}\"\n",
    "        page.page_in_doc = sum(1 for p in pages_info[:i] if p.doc_id == page.doc_id)\n",
    "\n",
    "    # Create LogicalDocument objects\n",
    "    logical_docs = []\n",
    "    unique_doc_ids = sorted(set(p.doc_id for p in pages_info))\n",
    "    for doc_id in unique_doc_ids:\n",
    "        pages_in_doc = [p for p in pages_info if p.doc_id == doc_id]\n",
    "        logical_doc = LogicalDocument(\n",
    "            doc_id=doc_id,\n",
    "            doc_type=pages_in_doc[0].doc_type,\n",
    "            page_start=pages_in_doc[0].page_num,\n",
    "            page_end=pages_in_doc[-1].page_num,\n",
    "            text=\"\\n\\n\".join([p.text for p in pages_in_doc])\n",
    "        )\n",
    "        logical_docs.append(logical_doc)\n",
    "\n",
    "    print(f\"‚úÖ Identified {len(logical_docs)} logical documents\")\n",
    "    for ld in logical_docs:\n",
    "        print(f\"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end} (ID: {ld.doc_id})\")\n",
    "\n",
    "    return pages_info, logical_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc37f6e-95ae-4c23-a99a-f0bf432b3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_with_llama_index(logical_doc: LogicalDocument,\n",
    "                           chunk_size: int = 500,\n",
    "                           chunk_overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Uses LlamaIndex's advanced chunking with metadata.\n",
    "    \"\"\"\n",
    "    # Create LlamaIndex document with metadata\n",
    "    doc = Document(\n",
    "        text=logical_doc.text,\n",
    "        metadata={\n",
    "            \"doc_id\": logical_doc.doc_id,\n",
    "            \"doc_type\": logical_doc.doc_type,\n",
    "            \"page_start\": logical_doc.page_start,\n",
    "            \"page_end\": logical_doc.page_end,\n",
    "            \"source\": f\"{logical_doc.doc_type}_document\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Use LlamaIndex's sentence splitter for better chunking\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        paragraph_separator=\"\\n\\n\",\n",
    "        separator=\" \",\n",
    "    )\n",
    "\n",
    "    # Create nodes (chunks) from document\n",
    "    nodes = splitter.get_nodes_from_documents([doc])\n",
    "\n",
    "    # Convert to our ChunkMetadata format for consistency\n",
    "    chunks_metadata = []\n",
    "    for i, node in enumerate(nodes):\n",
    "        chunk_meta = ChunkMetadata(\n",
    "            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
    "            doc_id=logical_doc.doc_id,\n",
    "            doc_type=logical_doc.doc_type,\n",
    "            chunk_index=i,\n",
    "            page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n",
    "            page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n",
    "            text=node.text\n",
    "        )\n",
    "        chunks_metadata.append(chunk_meta)\n",
    "\n",
    "    return chunks_metadata\n",
    "def process_all_documents(logical_docs: List[LogicalDocument],\n",
    "                         chunk_size: int=500,chunk_overlap:int=100) -> List[ChunkMetadata]:\n",
    "    \"\"\"\n",
    "    Process all logical documents into chunks with metadata.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for logical_doc in logical_docs:\n",
    "        chunks = chunk_with_llama_index(logical_doc,chunk_size,chunk_overlap)\n",
    "        logical_doc.chunks = chunks  # Store reference\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbee80b-d0cf-4236-9b14-b419ede82543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index_from_chunks(all_chunks,embedding_dim=384):\n",
    "  \"\"\"\n",
    "  Build a FAISS based index from document chunks.\n",
    "  \"\"\"\n",
    "  print(\"Building FAISS semantic index...\")\n",
    "  print(f\" Input: {len(all_chunks)} chunks\")\n",
    "  documents = []\n",
    "  for idx,chunk in enumerate(all_chunks):\n",
    "    doc = Document(\n",
    "        text = chunk.text,\n",
    "        metadata = {\n",
    "            \"chunk_id\":chunk.chunk_id,\n",
    "            \"doc_id\":chunk.doc_id,\n",
    "            \"doc_type\":chunk.doc_type,\n",
    "            \"page_start\":chunk.page_start,\n",
    "            \"page_end\":chunk.page_end,\n",
    "            \"chunk_index\":chunk.chunk_index\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "    # Creating FAISS index\n",
    "  faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "  vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "  storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "  vector_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "  print(f\"‚úÖ Created FAISS index with {len(documents)} chunks\")\n",
    "  return vector_index,vector_store\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19156d83-88a8-41b5-a2e0-4eb24b3d00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to store index\n",
    "index = None\n",
    "\n",
    "\n",
    "# Function we use to process the uploaded PDF\n",
    "def process_pdf(pdf_file,chunk_size=500,chunk_overlap=100):\n",
    "    global index,current_logical_docs\n",
    "    if pdf_file is None:\n",
    "        return \"‚ö†Ô∏è Please upload a PDF first!\",\"\"\n",
    "\n",
    "    try:\n",
    "      pages_info,logical_docs = extract_and_analyze_pdf(pdf_file)\n",
    "      current_logical_docs = logical_docs\n",
    "      print(f\"\\nüîÑ Processing {len(logical_docs)} logical documents...\")\n",
    "      all_chunks = process_all_documents(\n",
    "          logical_docs,\n",
    "          chunk_size=chunk_size,\n",
    "          chunk_overlap=chunk_overlap\n",
    "      )\n",
    "      index, vector_store = build_faiss_index_from_chunks(\n",
    "                              all_chunks,\n",
    "                              embedding_dim=384\n",
    "                              )\n",
    "\n",
    "      # Display information about the page\n",
    "      page_info_display = \"# üìÑ Document Analysis\\n\\n\"\n",
    "      page_info_display += \"## Logical Document Structure\\n\\n\"\n",
    "\n",
    "      for ld in logical_docs:\n",
    "            display_start = ld.page_start + 1\n",
    "            display_end = ld.page_end + 1\n",
    "            page_range = f\"{display_start}-{display_end}\" if ld.page_start != ld.page_end else str(display_start)\n",
    "            num_chunks = len(ld.chunks) if ld.chunks else 0\n",
    "            page_info_display += f\"**{ld.doc_type.upper()}** ({ld.doc_id})\\n\"\n",
    "            page_info_display += f\"  - Pages: {page_range}\\n\"\n",
    "            page_info_display += f\"  - Chunks: {num_chunks}\\n\\n\"\n",
    "      page_info_display += \"---\\n\\n\"\n",
    "      page_info_display += \"## üìä Processing Statistics\\n\\n\"\n",
    "      page_info_display += f\"- **Total pages:** {len(pages_info)}\\n\"\n",
    "      page_info_display += f\"- **Logical documents:** {len(logical_docs)}\\n\"\n",
    "      page_info_display += f\"- **Total chunks:** {len(all_chunks)}\\n\"\n",
    "      page_info_display += f\"- **Chunk size:** {chunk_size} tokens\\n\"\n",
    "      page_info_display += f\"- **Chunk overlap:** {chunk_overlap} tokens\\n\"\n",
    "      success_msg = f\"‚úÖ Successfully processed {len(pages_info)} page(s) and created {len(all_chunks)} chunks!\\n\\nYou can now ask questions about the document.\"\n",
    "      return success_msg, page_info_display\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error processing PDF: {str(e)}\\n\\nPlease make sure the file is a valid PDF.\"\n",
    "        return error_msg,\"\"\n",
    "\n",
    "# Function to handle user questions\n",
    "def answer(question, chat_history):\n",
    "    global index\n",
    "\n",
    "    if index is None:\n",
    "        return chat_history + [{\"role\": \"assistant\", \"content\": \"‚ö†Ô∏è Please upload and process a PDF document first!\"}], \"\"\n",
    "\n",
    "    if not question or not question.strip():\n",
    "        return chat_history, \"\"\n",
    "\n",
    "    try:\n",
    "        response_synthesizer = get_response_synthesizer(response_mode=\"compact\",\n",
    "                                                        text_qa_template = PromptTemplate(\"You are a helpful and friendly document answering assistant. \"\n",
    "    \"Using only the provided context, answer the question accurately.\\n\\n\"\n",
    "    \"If the context does not contain enough information, reply 'Not enough context to provide this answer'.\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    ))\n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            response_synthesizer=response_synthesizer\n",
    "        )\n",
    "        response = query_engine.query(question)\n",
    "        print(response.source_nodes)\n",
    "        response_text = str(response)\n",
    "        is_no_context = \"not enough context to provide this answer\" in response_text.lower()\n",
    "        if hasattr(response,'source_nodes') and response.source_nodes and not is_no_context:\n",
    "            top_node = response.source_nodes[0]\n",
    "            meta = top_node.metadata\n",
    "            page_start = meta.get('page_start',0) + 1\n",
    "            page_end = meta.get('page_end',page_start) + 1\n",
    "            doc_type = meta.get('doc_type','unknown')\n",
    "            if page_start == page_end:\n",
    "              source_str = f\"Page {page_start} ({doc_type})\"\n",
    "            else:\n",
    "              source_str = f\"Pages {page_start}-{page_end} ({doc_type})\"\n",
    "            response_text += f\"\\n\\nüìç *Sources: {source_str}*\"\n",
    "        chat_history = chat_history + [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": response_text}\n",
    "        ]\n",
    "        return chat_history, \"\"  # we are returning the empty string to clear input\n",
    "    except Exception as e:\n",
    "        chat_history = chat_history + [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": f\"‚ùå Error: {str(e)}\"}\n",
    "        ]\n",
    "        return chat_history, \"\"\n",
    "\n",
    "def save_chat(chat_history):\n",
    "  if not chat_history:\n",
    "    return \"‚ö†Ô∏è No chat to save!\"\n",
    "  timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "  filename = f\"chat_{timestamp}.txt\"\n",
    "  with open(filename,'w') as f:\n",
    "    f.write(f\"Chat History -{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    for msg in chat_history:\n",
    "      f.write(f\"{msg['role'].upper()}:\\n\")\n",
    "      f.write(f\"{msg['content']}\\n\")\n",
    "      f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "  return f\"Saved to : {filename}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87f05c-9845-4ab7-ada9-76a363dc2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI\n",
    "with gr.Blocks(title=\"PDF Q&A\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# üìö PDF Q&A with Advanced Document Analysis\")\n",
    "    gr.Markdown(\"Upload a PDF with multiple document types. The system automatically detects, groups, and chunks them.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Chat with Your Documents\",\n",
    "                height=300,\n",
    "                type=\"messages\",\n",
    "            )\n",
    "            user_input = gr.Textbox(\n",
    "                placeholder=\" Ask a question...\",\n",
    "                show_label=False,\n",
    "                container=False\n",
    "            )\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\", size=\"sm\")\n",
    "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", size=\"sm\")\n",
    "                save_btn = gr.Button(\"üíæ Save Chat\",size=\"sm\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            pdf_input = gr.File(\n",
    "                label=\"üìÑ Upload PDF\",\n",
    "                file_types=[\".pdf\"],\n",
    "                type=\"filepath\"\n",
    "            )\n",
    "            process_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\")\n",
    "            status_box = gr.Textbox(\n",
    "                label=\"Status\",\n",
    "                interactive=False,\n",
    "                lines=2,\n",
    "                max_lines=2\n",
    "            )\n",
    "            page_info_box = gr.Markdown(\n",
    "                label=\"üìã Analysis\",\n",
    "                value=\"Upload and process a document to see the analysis.\"\n",
    "            )\n",
    "\n",
    "\n",
    "    # Event Handlers\n",
    "    process_btn.click(\n",
    "        fn=process_pdf,\n",
    "        inputs=[pdf_input],\n",
    "        outputs=[status_box,page_info_box],\n",
    "        queue=True\n",
    "    )\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=answer,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=True\n",
    "    )\n",
    "\n",
    "    user_input.submit(\n",
    "        fn=answer,\n",
    "        inputs=[user_input, chatbot],\n",
    "        outputs=[chatbot, user_input],\n",
    "        queue=True\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], \"\"),\n",
    "        outputs=[chatbot, user_input]\n",
    "    )\n",
    "    save_btn.click(fn=save_chat,inputs=chatbot,outputs=status_box)\n",
    "demo.queue()\n",
    "demo.launch(debug=False,share=True,inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
